{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b66a8c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imutils in c:\\users\\rasaa\\onedrive\\desktop\\prepinsta project\\opencv project\\facemask\\lib\\site-packages (0.5.4)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85d57238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\rasaa\\onedrive\\desktop\\prepinsta project\\opencv project\\facemask\\lib\\site-packages (4.7.0.72)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\rasaa\\onedrive\\desktop\\prepinsta project\\opencv project\\facemask\\lib\\site-packages (from opencv-python) (1.23.5)\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82316399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f313e420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\rasaa\\onedrive\\desktop\\prepinsta project\\opencv project\\facemask\\lib\\site-packages (9.4.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e0c703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89508181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4651e5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "196b9cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "668f329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07138157",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "460ce0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\rasaa\\onedrive\\desktop\\prepinsta project\\opencv project\\facemask\\lib\\site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\rasaa\\onedrive\\desktop\\prepinsta project\\opencv project\\facemask\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\rasaa\\onedrive\\desktop\\prepinsta project\\opencv project\\facemask\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\rasaa\\onedrive\\desktop\\prepinsta project\\opencv project\\facemask\\lib\\site-packages (from matplotlib) (4.39.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\rasaa\\onedrive\\desktop\\prepinsta project\\opencv project\\facemask\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\rasaa\\onedrive\\desktop\\prepinsta project\\opencv project\\facemask\\lib\\site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rasaa\\onedrive\\desktop\\prepinsta project\\opencv project\\facemask\\lib\\site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\rasaa\\onedrive\\desktop\\prepinsta project\\opencv project\\facemask\\lib\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\rasaa\\onedrive\\desktop\\prepinsta project\\opencv project\\facemask\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\rasaa\\onedrive\\desktop\\prepinsta project\\opencv project\\facemask\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rasaa\\onedrive\\desktop\\prepinsta project\\opencv project\\facemask\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0af3b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76ba446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_lr=1e-4\n",
    "bs=32\n",
    "epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb1b093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory='datasets/mask_detection_video'\n",
    "categories=['with_mask','without_mask']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b59b05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rasaa\\OneDrive\\Desktop\\prepinsta project\\OpenCV project\\facemask\\Lib\\site-packages\\PIL\\Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data=[]\n",
    "labels=[]\n",
    "for category in categories:\n",
    "    path=os.path.join(directory,category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path=os.path.join(path,img)\n",
    "        image=load_img(img_path,target_size=(224,224))\n",
    "        \n",
    "        \n",
    "        image=img_to_array(image)\n",
    "        image=preprocess_input(image)\n",
    "        data.append(image)\n",
    "        labels.append(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0d8c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb=LabelBinarizer()\n",
    "labels=lb.fit_transform(labels)\n",
    "labels=to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd5dd207",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array(data,dtype='float32')\n",
    "labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac5a818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX,testX,trainY,testY)=train_test_split(data,labels,test_size=0.2,stratify=labels,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b0eb881",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug=ImageDataGenerator(rotation_range=20,\n",
    "                      zoom_range=0.15,\n",
    "                      width_shift_range=0.2,\n",
    "                      height_shift_range=0.2,\n",
    "                      shear_range=0.15,\n",
    "                      horizontal_flip=True,\n",
    "                      fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86c7d12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "baseModel=MobileNetV2(weights='imagenet',include_top=False,\n",
    "                     input_tensor=Input(shape=(224,224,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38ac9ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "headModel=baseModel.output\n",
    "headModel=AveragePooling2D(pool_size=(7,7))(headModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f65ed8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "headModel=Flatten(name='flatten')(headModel)\n",
    "headModel=Dense(128,activation='relu')(headModel)\n",
    "headModel=Dropout(0.5)(headModel)\n",
    "headModel=Dense(2,activation='softmax')(headModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a6d3f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(inputs=baseModel.input,outputs=headModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50725981",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in baseModel.layers:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38a5c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=Adam(learning_rate=init_lr,decay=init_lr/epochs)\n",
    "model.compile(optimizer=opt,metrics=['accuracy'],loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fe1427d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "95/95 [==============================] - 104s 1s/step - loss: 0.4112 - accuracy: 0.8556 - val_loss: 0.1535 - val_accuracy: 0.9896\n",
      "Epoch 2/20\n",
      "95/95 [==============================] - 104s 1s/step - loss: 0.1545 - accuracy: 0.9657 - val_loss: 0.0813 - val_accuracy: 0.9883\n",
      "Epoch 3/20\n",
      "95/95 [==============================] - 97s 1s/step - loss: 0.1103 - accuracy: 0.9710 - val_loss: 0.0595 - val_accuracy: 0.9909\n",
      "Epoch 4/20\n",
      "95/95 [==============================] - 98s 1s/step - loss: 0.0732 - accuracy: 0.9832 - val_loss: 0.0488 - val_accuracy: 0.9909\n",
      "Epoch 5/20\n",
      "95/95 [==============================] - 91s 962ms/step - loss: 0.0755 - accuracy: 0.9743 - val_loss: 0.0415 - val_accuracy: 0.9909\n",
      "Epoch 6/20\n",
      "95/95 [==============================] - 87s 915ms/step - loss: 0.0626 - accuracy: 0.9822 - val_loss: 0.0390 - val_accuracy: 0.9922\n",
      "Epoch 7/20\n",
      "95/95 [==============================] - 86s 907ms/step - loss: 0.0534 - accuracy: 0.9848 - val_loss: 0.0343 - val_accuracy: 0.9909\n",
      "Epoch 8/20\n",
      "95/95 [==============================] - 87s 913ms/step - loss: 0.0502 - accuracy: 0.9845 - val_loss: 0.0329 - val_accuracy: 0.9922\n",
      "Epoch 9/20\n",
      "95/95 [==============================] - 87s 920ms/step - loss: 0.0495 - accuracy: 0.9852 - val_loss: 0.0336 - val_accuracy: 0.9922\n",
      "Epoch 10/20\n",
      "95/95 [==============================] - 93s 979ms/step - loss: 0.0440 - accuracy: 0.9865 - val_loss: 0.0317 - val_accuracy: 0.9922\n",
      "Epoch 11/20\n",
      "95/95 [==============================] - 92s 969ms/step - loss: 0.0360 - accuracy: 0.9895 - val_loss: 0.0301 - val_accuracy: 0.9922\n",
      "Epoch 12/20\n",
      "95/95 [==============================] - 91s 952ms/step - loss: 0.0336 - accuracy: 0.9914 - val_loss: 0.0303 - val_accuracy: 0.9935\n",
      "Epoch 13/20\n",
      "95/95 [==============================] - 91s 956ms/step - loss: 0.0334 - accuracy: 0.9911 - val_loss: 0.0287 - val_accuracy: 0.9922\n",
      "Epoch 14/20\n",
      "95/95 [==============================] - 91s 956ms/step - loss: 0.0360 - accuracy: 0.9885 - val_loss: 0.0294 - val_accuracy: 0.9922\n",
      "Epoch 15/20\n",
      "95/95 [==============================] - 95s 996ms/step - loss: 0.0330 - accuracy: 0.9904 - val_loss: 0.0296 - val_accuracy: 0.9922\n",
      "Epoch 16/20\n",
      "95/95 [==============================] - 87s 918ms/step - loss: 0.0317 - accuracy: 0.9904 - val_loss: 0.0270 - val_accuracy: 0.9935\n",
      "Epoch 17/20\n",
      "95/95 [==============================] - 84s 888ms/step - loss: 0.0299 - accuracy: 0.9921 - val_loss: 0.0277 - val_accuracy: 0.9935\n",
      "Epoch 18/20\n",
      "95/95 [==============================] - 85s 890ms/step - loss: 0.0273 - accuracy: 0.9924 - val_loss: 0.0250 - val_accuracy: 0.9935\n",
      "Epoch 19/20\n",
      "95/95 [==============================] - 90s 946ms/step - loss: 0.0271 - accuracy: 0.9911 - val_loss: 0.0289 - val_accuracy: 0.9922\n",
      "Epoch 20/20\n",
      "95/95 [==============================] - 88s 922ms/step - loss: 0.0278 - accuracy: 0.9921 - val_loss: 0.0250 - val_accuracy: 0.9935\n"
     ]
    }
   ],
   "source": [
    "H=model.fit(aug.flow(trainX,trainY,batch_size=bs),\n",
    "            steps_per_epoch=len(trainX)//bs,\n",
    "            validation_data=(testX,testY),\n",
    "            validation_steps=len(testX)//bs,\n",
    "            epochs=epochs\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88975c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 16s 603ms/step\n"
     ]
    }
   ],
   "source": [
    "pred=model.predict(testX,batch_size=bs)\n",
    "pred_classes = np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88408998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   with_mask       0.99      0.99      0.99       383\n",
      "without_mask       0.99      0.99      0.99       384\n",
      "\n",
      "    accuracy                           0.99       767\n",
      "   macro avg       0.99      0.99      0.99       767\n",
      "weighted avg       0.99      0.99      0.99       767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(testY.argmax(axis=1),pred_classes,target_names=lb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79b29e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mask_detector_new.model',save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1b4b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facemask",
   "language": "python",
   "name": "facemask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
